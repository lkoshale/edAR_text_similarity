# -*- coding: utf-8 -*-
"""MaLSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HKhjxq5PUXwYo3QUBJXdEWKNnvz4znig
"""

!pip3 install --quiet "tensorflow>=1.7"
!pip3 install --quiet tensorflow-hub
!pip3 install --quiet seaborn

!rm edAR_text_similarity
!git clone https://github.com/lkoshale/edAR_text_similarity.git

import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns
from time import time
from gensim.models import KeyedVectors
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import itertools
import datetime
from keras.preprocessing.sequence import pad_sequences
from keras.models import Model
from keras.layers import Input, Embedding, LSTM, Lambda
import keras.backend as K
from keras.optimizers import Adadelta
from keras.callbacks import ModelCheckpoint
from keras import layers

module_url = "https://tfhub.dev/google/universal-sentence-encoder-large/3"

embed = hub.Module(module_url)

df_train = pd.read_csv('edAR_text_similarity/data/train1.csv')
df_test = pd.read_csv('edAR_text_similarity/data/test1.csv')

df_train.head()

df_test.head()

X = df_train.iloc[:, :-1].values
y = df_train.iloc[:, 5].values

X_test = df_test.iloc[:,:-1].values

from sklearn.model_selection import train_test_split
X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size = 0.2, random_state = 0)

X_train_msg1=[]
X_train_msg2=[]
for i in range(0,X_train.shape[0]):
  X_train_msg1.append(X_train[i,3])
  X_train_msg2.append(X_train[i,4])

tf.logging.set_verbosity(tf.logging.ERROR)

with tf.Session() as session:
  session.run([tf.global_variables_initializer(), tf.tables_initializer()])
  X_train_msg1_embed = session.run(embed(X_train_msg1))
  X_train_msg2_embed = session.run(embed(X_train_msg2))

X_train_msg1_embed = np.asarray(X_train_msg1_embed, dtype=object)
X_train_msg2_embed = np.asarray(X_train_msg2_embed, dtype=object)

X_train_msg1_embed = X_train_msg1_embed[:,np.newaxis]
X_train_msg2_embed = X_train_msg2_embed[:,np.newaxis]

X_train_msg1_embed.shape[1]

X_validation_msg1=[]
X_validation_msg2=[]
for i in range(0,X_validation.shape[0]):
  X_validation_msg1.append(X_validation[i,3])
  X_validation_msg2.append(X_validation[i,4])

tf.logging.set_verbosity(tf.logging.ERROR)

with tf.Session() as session:
  session.run([tf.global_variables_initializer(), tf.tables_initializer()])
  X_validation_msg1_embed = session.run(embed(X_validation_msg1))
  X_validation_msg2_embed = session.run(embed(X_validation_msg2))

X_validation_msg1_embed = X_validation_msg1_embed[:,np.newaxis]
X_validation_msg2_embed = X_validation_msg2_embed[:,np.newaxis]

X_validation_msg1_embed

X_test_msg1=[]
X_test_msg2=[]
for i in range(0,X_test.shape[0]):
  X_test_msg1.append(X_test[i,3])
  X_test_msg2.append(X_test[i,4])

tf.logging.set_verbosity(tf.logging.ERROR)

with tf.Session() as session:
  session.run([tf.global_variables_initializer(), tf.tables_initializer()])
  X_test_msg1_embed = session.run(embed(X_test_msg1))
  X_test_msg2_embed = session.run(embed(X_test_msg2))

X_test_msg1_embed = X_test_msg1_embed[:,np.newaxis]
X_test_msg2_embed = X_test_msg2_embed[:,np.newaxis]

len(y_validation)

n_hidden = 50
gradient_clipping_norm = 1.25
batch_size = 64
n_epoch = 25

def exponent_neg_manhattan_distance(left, right):
    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))

def UniversalEmbedding(x):
    return embed(tf.squeeze(tf.cast(x, tf.string)), signature="default", as_dict=True)["default"]

left_input = Input(shape=(1,512), dtype='float32')
right_input = Input(shape=(1,512), dtype='float32')

shared_lstm = LSTM(n_hidden)

left_output = shared_lstm(left_input)
right_output = shared_lstm(right_input)

malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])

malstm = Model([left_input, right_input], [malstm_distance])


optimizer = Adadelta(clipnorm=gradient_clipping_norm)

malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])

training_start_time = time()

malstm_trained = malstm.fit([X_train_msg1_embed, X_train_msg2_embed], y_train, batch_size=batch_size, nb_epoch=n_epoch,
                            validation_data=([X_validation_msg1_embed, X_validation_msg2_embed], y_validation))

print("Training time finished.\n{} epochs in {}".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))

malstm.save_weights('./model.h5')

with tf.Session() as session:
  K.set_session(session)
  session.run(tf.global_variables_initializer())
  session.run(tf.tables_initializer())
  malstm.load_weights('./model.h5')  
  predicts = malstm.predict([X_test_msg1_embed, X_test_msg2_embed], batch_size=32)

predicts[1]

y_test = df_test.iloc[:,5].values

for i in range(0,199):
  if predicts[i]>0.5:
    predicts[i] = 1
  else:
    predicts[i] = 0

predicts

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, predicts)

cm
